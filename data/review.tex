\chapter{文献综述}



\section{强化学习}
\label{sec:review_rl}

\section{分布式强化学习}

深度强化学习（DRL）已经在一系列任务中取得了显著的成功，从游戏\cite{silver2016mastering,silver2017mastering}到现实世界的问题\cite{jumper2021highly}。更重要的是，大规模的强化学习已经能够在一些具有挑战性的游戏上达到与人类接近甚至超越人类的水平，如StartCraft 2\cite{vinyals2019grandmaster}和Dota\cite{berner2019dota}。DRL严重依赖试错和经验评估，需要智能体进行大量探索，这使得与游戏环境交互的时间成为了模型训练的瓶颈。因此，该领域的推进迫切需要分布式方法来提高探索效率。

同时，目前面向深度学习的分布式系统和算法取得了极大的进展，其中包括梯度下降算法的分布式形式以及分布式的深度学习框架\cite{goyal2017accurate, you2017scaling}，使得大规模的模型训练成为可能。因此，研究人员提出并开发了面向深度强化学习的分布式架构，使智能体能够更快地从经验中学习，利用探索策略，甚至有能力同时学习不同的任务集。

强化学习的特点在于，模型训练和数据采集是同时进行的。分布式强化学习旨在将两者并行化处理，以及为两者实现并行加速。因此如何确保数据的实效性，协调训练与模拟的速度，采样训练等，都是分布式强化学习算法需要解决的问题。

强化学习任务的问题定义，以及一些典型的强化学习算法已经在\ref{sec:review_rl}描述，本节的后续部分将探讨如何将这些算法进行分布式加速。本章节将只涉及单智能体算法，分布式多智能体算法不在讨论的范围。

\subsection{GORILA}

GORILA（The General Reinforcement Learning Architecutre）\cite{nair2015massively}是一个异步分布式强化学习架构。Gorila在算法上与DQN有相似之处，但有多个执行器和学习器，每个都在一台机器内运行，并在每个机器内存储一个Q网络。它利用DistBelief方法\cite{dean2012large}来计算随机梯度下降。

GORILA框架架构由4个主要部分组成，分别为参数服务器、执行器、学习器和经验回放缓冲区。参数服务器是Q网络参数$\theta$的真实来源，执行器和学习器各自存储一个$\theta$的副本，因此参数服务器试图使执行器和学习器的$\theta$保持最新。每个执行器都存储着参数$\theta$，根据它来采取行动。它只是用于行动，不参与随机梯度下降程序。所以它与参数服务器的更新同步。执行器将他们的经验$(s,a,s',a')$发送到经验回放缓冲区中，缓冲区可以放在一个独立的分布式数据库中，也可以放在执行器内部。学习器从缓冲器中抽取经验并执行随机梯度下降更新。在DQN中，每个学习器保持一个可学习的Q网络，参数为$\theta$，以及一个参数固定的目标Q网络，参数为$\theta^-$。目标网络在$N$次迭代中被更新一次，在DQN中就是如此。学习器的责任是根据从缓冲区采样的经验来计算损失。但是与DQN不同的是，学习器并不更新Q网络，而是将产生的梯度发送到参数服务器上。只有当参数服务器更新$\theta$时，改进的Q网络才会被同步，因为还有其他学习器在$\theta$上发布更新，而且发出的更新不一定有效。参数服务器包含一个DistBelief网络。如上所述，它负责根据来自学习器的传入梯度更新网络参数$\theta$，并将网络参数发送到学习器和执行器的Q网络。参数服务器丢弃“过期”的梯度更新，使得网络参数即使在进程和通信延迟和失败的情况下也是可靠的。这个架构的几个变体被归类为GORILA。学习器和执行器的数量是任意的，学习器和执行器可以是“捆绑”的，学习器和执行器数量相同，两两成对放在一台机器里。这个变体减少了通信开销，所以它的性能更好。

\subsection{A3C和A2C}

在GORILA中，执行器与学习器的数据传递通过经验回放缓冲区进行，这也限制了其适用范围仅为离轨策略（off-policy）算法。A3C（Asynchronous Advantage Actor-Critic）\cite{mnih2016asynchronous}框架为同轨策略（on-policy）算法进行了分布式结构设计。

A3C包含了一个全局的策略神经网络，以及多个智能体，每个智能体都有自己的神经网络参数。智能体与它的环境副本进行互动并使用获得的经验训练自己的神经网络。当智能体计算得到当前的神经网络梯度后，将这一梯度提交给全局的策略神经网络节点；该节点更新参数后将最新的神经网络参数返回给该智能体，以更新其网络参数。每个智能体与环境的交互和网络参数的更新都是异步的，这也导致了这种网络参数更新策略并不是最优的。

为了达到最优的网络更新效果，A2C（Advantage Actor-Critic）将全局策略神经网络的参数进行同步更新：接受当前迭代中来自所有智能体的梯度后统一更新并返回更新后的参数，然后在下一次迭代中，每个智能体从同样的策略开始。同步梯度更新解决了提到的不一致问题，在大批量的情况下效果更好，但却缺乏智能体的独立性。

\subsection{Ape-X和R2D2}

事实上，类似于GORILA的框架中，随着执行器数量的增加，模型能够收集足够多的训练数据。在这些样本中，已知的样本占绝大部分。价值神经网络已经能够较好的估计这些样本的价值，因此重复训练是冗余的。而对于在随机探索中获取的未知的样本相对稀少，而其价值不能被很好的估计。在实际采样中，我们需要根据重要性为这些样本分配采样权重，使得稀有的有效样本能够具备更大的被采样概率。优先经验回放缓冲区（Prioritized Experience Replay, PER）\cite{schaul2015prioritized}让优先级更高的样本更有概率被采集到，而优先级低的样本被采集的概率则降低。
% 算法定义每个样本的优先级为其时序差分误差（TD-error）。为保证所有样本都有概率被采集，优先级在原基础上加上了一个很小的值$\epsilon$。
PER算法的细节将在xxx描述并加以讨论。

论文\cite{horgan2018distributed}提出分布式强化学习框架Ape-X。其结构相比与GORILA更为简洁，移去了参数服务器，仅使用一个学习器消化来自所有执行器的样本。借助优先经验回放缓冲区，Ape-X在Atari电子游戏基线上获得了远超GORILA的实验结果。Ape-X框架以DQN算法为基础，组合了DQN的多种变体，包括双Q学习\cite{hasselt2010double, van2016deep}算法，多步回报时序差分估计，并使用了Dueling DQN网络结构\cite{wang2016dueling}作为价值函数近似器。为了解决连续动作空间的任务，Ape-X还实现了深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）\cite{lillicrap2015continuous}算法。

Ape-X的后续工作R2D2（Recurrent Experience Replay Distributed Reinforcement Learning）\cite{kapturowski2018recurrent}加入了循环神经网络RNN结构，扩展了马尔可夫决策过程的假设，使当前状态的价值估计由更长的历史状态和动作决定。对于R2D2来说，经验回放缓冲区存储的不再是$(s,a,r,s')$四元组，而是预设长度的$(s,a,r)$序列。其经验的优先级由序列中最大优先级和平均优先级共同决定。

\subsection{SEED RL}

在上述的所有分布式框架中，执行器一侧的策略神经网络推理都是在CPU上执行的，这比使用加速器的计算效率低很多。若是在执行器上部署GPU来加速推理，则会限制执行器的数量。随着网络模型变得庞大，这个问题变得更加关键。此外，每个执行器在一台机器上操作两个不同的任务：模型推理和环境渲染。这导致了低效益的资源利用。因为这两个任务并不相似。另一个问题是，这些分布式框架中，模型参数和经验轨迹在执行器和学习器之间传输，因此所需的带宽可能成为一个限制性因素。

SEED RL（Scalable and Efficient Deep-RL）\cite{espeholt2019seed}是一个高效的利用资源和加速器的分布式强化学习框架。其克服这些缺点的关键思想是，模型推理和轨迹收集是在学习器中进行的，而非在执行器。整个设计在某种程度上是一个具有远程环境的单机结构。因此我们可以使用如GPU和TPU的加速器实现批量推理。SEED RL的完整流程可以描述为：执行器将与环境交互获得的状态和奖励传递给学习器。学习器在聚合了一批状态后使用其策略神经网络实现批量推理，并将动作返回给各个执行器。同时学习器还将$(s,a,r,s')$四元组保存在本地存储用于网络训练。执行器在获得推理得到的动作后执行动作，进入下一轮迭代。

然而， 在这个模型中可能出现延迟问题。将推理放置在远端，当通信的时间开销超过本地推理的时间开销时，执行器则会出现延迟。为了减少这种延迟，SEED RL引入了一个基于gRPC（一个高性能的开源RPC框架）的高效库，包括异步流RPC（每个执行器与学习器的连接都保持开放）。在这个框架中，采用了一个批处理模块，以一种良好的组织方式对多个角色的推理调用进行批处理。此外，在行为体可以在同一台机器上的情况下，该框架利用Unix域套接字，因此可以最小化延迟和CPU开销。基于这一通信框架，SEED RL在单台机器上实现了每秒一百万次的查询，执行器的数量可以增加到数千台机器，学习器可以扩展到数千个核心，使其有可能以每秒数百万帧的速度训练。

\subsection{其他分布式强化学习框架}

A3C与A2C框架由于采用了同轨策略梯度算法Actor-Critic，因此其样本利用率较低。IMPALA\cite{espeholt2018impala}沿用了A3C的分布式架构，并在其上参考Retrace\cite{munos2016safe}加入了V-trace方法对历史轨迹得到的梯度进行同轨修正。这使得IMPALA能够维护一个较小规模的经验回放缓冲区，提升样本了利用率。

Distributed PPO（Proximal Policy Optimization）\cite{heess2017emergence}将PPO\cite{schulman2017proximal}算法进行扩展，得到了一个类似于A3C结构的分布式架构。

\section{分层强化学习}

强化学习智能体的目标是搜索最佳策略，使得在这一最佳策略之下智能体能够最大化累计奖励的期望值。在探索状态空间和动作空间以搜寻最优策略时，智能体将会得到各种轨迹。这些轨迹的期望长度被称作这一任务的视野（Horizon）。视野在强化学习理论中被定义为无限的，但是实际任务场景中通常是有限的。当状态空间和动作空间很大，任务的视野很远时，使用标准的强化学习方法进行探索就变得很有挑战性。

分层强化学习提供了一种机制，将具有挑战性的任务分解成更简单的子任务。在这样的层次结构中，最高级别的策略的动作空间通常被定义为选择一个子任务来分配给低级别策略执行。这个策略通过以合适的顺序选择子任务来完成主任务，以获得主任务的奖励。在层次结构的较低层次，又高层策略选择的子任务本身就是一个强化学习问题。底层策略学习使用与之相关的内在奖励来执行该子任务（也可选择加入主任务奖励）。最底层的策略的动作空间，即为主任务的动作空间。

在一种分类方案下，分层强化学习可以被分为封建分层结构，子策略选择结构，独立子任务探索，以及可迁移分层强化学习。

\subsection{封建分层结构}

Dayan和Hinton于1993年提出了基础的封建分层强化学习结构\cite{NIPS1992_d14220ee}，被称为封建强化学习（Feudal Reinforcement Learning）。封建强化学习简要描述如下：一个较高层次的管理者设置了一个子任务，由较低层次的工人执行。这是一种成对的关系，如果层次结构有两层以上，工人就会成为它下面那一层的管理者。管理者通过一个子目标将子任务传达给工人，其中子目标是原始或抽象状态空间中的一个状态。工人的目标是要达到给定的子目标。每个级别的工人都是一个通用策略。任务奖励只有最高级别的管理者可以观察到，而其他级别的工人则利用达到子目标的奖励来学习。本文讨论的其他封建分层方法都是基于上述封建强化学习的概念。

分层DQN（Hierarchical DQN）\cite{kulkarni2016hierarchical}算法是一种深度分层强化学习算法，由分别代表管理者和工人的两层深度Q网络组成。管理者网络从一组预定义的子目标中选择一个子目标。它是根据环境奖励来学习的。工人的网络则是将子目标作为输入，并选择游戏环境动作空间中的动作实现子目标。它是根据子目标的奖励来学习的。一个子目标要么是原始状态，要么是状态的抽象表示。

工人智能体在学习过程中不断变化，也就意味着管理者的状态转换是非平稳的。为了在非平稳环境中实现数据的高效学习，HIRO（Hierarchical Reinforcement Learning with Off-policy Correction）在两级分层结构下，提出将子目标重新标记以解决非平稳问题。在工人的历史数据中，包含了子目标$g_{old}$和其对应的轨迹$\tau$。而经过网络更新后的工人神经网络在$g_{old}$下将不再执行轨迹$\tau$。HIRO提出的离轨修正方案在分布中寻找一个新的子目标$g_{new}$，使得在这一子目标$g_{new}$下，当前策略能够得到与轨迹$\tau$最相似的轨迹。

与HIRO同时，分层Actor-Critic（Hierarchical Actor-Critic）\cite{levy2017learning}算法出现

% Dayan和Hinton[19]介绍了基础性的封建等级制度，称为封建强化学习（Feudal Reinforcement Learning，Feudal RL）。封建强化学习简要描述如下：一个较高层次的管理者设置了一个子任务，由较低层次的工人执行。这是一种成对的关系，如果层次结构有两层以上，工人就会成为它下面那一层的经理。管理者通过一个子目标将子任务传达给工作者，其中子目标只是原始或抽象状态空间中的一个状态。工作者的目标是要达到给定的子目标。每个级别的工作者都是一个通用策略。任务奖励只能由最高级别的管理者观察，而其他级别的工人则利用达到子目标的奖励来学习。作者在一个网格世界环境中的迷宫导航任务上评估了Feudal RL，该任务使用不同空间距离的预定义状态作为子目标。Feudal RL比标准的Q-learning更快地收敛到通往迷宫中主要目标状态的较短路径。本文讨论的其他封建等级制度方法都是基于上述封建RL的概念。
% Kulkarni等人[51]提出了一种深层HRL方法，由代表经理和工人的深层Q网络（DQNs）[68]的两级层次组成。经理网络从一组预定义的子目标中选择一个子目标。它是通过任务奖励来学习的。工人网络将子目标作为输入，并选择原始的行动来实现子目标。它是利用达到子目标的奖励来学习的。一个子目标要么是原始状态，要么是一个状态的抽象表示，例如，从图像中提取的物体，图像代表原始状态。用于评估的任务之一是Atari Montezuma's Revenge，在这个任务中，标准的DQN在200万个时间步长的收集奖励方面几乎没有进展。然而，随着训练的进行，带有预定义子目标的Deep HRL逐渐获得更高的奖励。
% 同时学习多层次的政策会导致非平稳性的问题[54, 69]。这意味着状态-行动-下一个状态的转换数据，是由执行较低级别的策略产生的，并被较高级别的策略所观察到，即使是在环境的同一状态下选择的同一个子目标，在不同的时间实例中也是不同的。这是因为低层策略不是静止的，它对（状态，子目标）对的反应在学习过程中会发生变化。非稳态性可能会导致许多无用的数据样本，因此需要解决这个问题以实现数据高效的学习。
% 为了解决非平稳性的问题，Nachum等人[69]提出了一个两级封建层次结构，并有一个子目标重新标记的机制。这种方法被称为Hierarchical Rein-
% forcement Learning with Off-policy Correction（HIRO）。HIRO中的子目标重标可以描述如下。一个两级的HRL代理与环境互动并收集经验数据。这些数据由高层策略的过渡图元组成，其中r是任务奖励。这里，c是实现每个子目标的固定时间跨度，r дt是实现дt的奖励。这些数据随后被用于训练HRL代理的分层策略。然而，如果代理人在c个时间段后没有实现дt，那么在过渡数据中，该子目标将被重新标记为另一个子目标дt′，该子目标是从使观察到的过渡的概率最大化的子目标分布中抽取的。然后，高层策略将дt′作为其事后的输出，这与观察到的过渡有更好的关联。通过这种方式，HIRO减少了有效的非平稳性。在HIRO中，一个子目标是通过从原始状态空间的一个状态中选择特征来定义的。HIRO在MuJoCo[97]连续控制任务上进行了评估，显示出比标准RL和其他少数HRL方法更好的表现[25, 99]。
% 与HIRO同时，Levy等人[54]提出了一种叫做分层演员批评（HAC）的方法，它也通过子目标的重新标记来解决非平稳性问题。在这个方案中，高层数据中的输出子目标和低层数据中的输入子目标被替换为代理在事后取得的实际状态，而不是像HIRO那样基于概率的抽样，抽出一个新的子目标。这种简单的方案也使层次结构扩展到两级以上成为可能。HAC在MuJoCo[97]连续控制任务上进行了评估，显示出比标准RL和HIRO更好的性能。此外，作者发现在这些任务上，三层的层次结构比两层的层次结构表现得更好。
% 封建的层次结构也可以通过使用自然语言指令而不是子目标状态来实现。Jiang等人[41]提出了一种叫做 "语言层次抽象"（HAL）的方法。他们假设从状态s∈S到语言指令4 l∈L的映射是由人类监督者给出的，或者使用预定义程序生成的，是指令空间L中指令的概率分布。在此基础上，定义了一个布尔函数Ψ : S × L → (0, 1)。现在，如果高层政策选择了一条指令l，那么Ψ(s, l) = 1的所有状态都是该指令的子目标状态。就本文使用的符号而言，一条语言指令l是一个子任务ω，所有满足Ψ(s,l)=1的状态都属于ω的子目标集，即дω。语言指令用递归神经网络[16]进行编码，其输出作为输入给下层策略。用于训练下层策略的子任务奖励rω是实现满足给定指令的状态的二进制奖励。HAL还使用一种叫做 "后知后觉指令再标记 "的机制来处理非平稳性。HAL在MuJoCo连续控制领域[100]的对象排列任务上进行了评估，与CLEVR语言数据集[42]相结合，发现其性能优于标准RL和少数其他HRL方法[3, 69]。

% 在一个封建的UNI层次结构中，所有层次的子目标空间和通用策略都是以统一的方式学习的。
% Vezhnevets等人[99]提出了一个神经网络的封建层次结构，其中一个被称为 "管理者 "的较高层次的网络在学习到的潜在子目标空间中对一个子目标进行采样。子目标可以是潜在空间中的一个点，也可以是代表潜在空间中的一个方向的单位向量。子目标被称为 "工人 "的低级网络作为输入，该网络必须学习一个策略来实现子目标，使用与子目标的距离作为奖励。工作者是使用从基于子目标的奖励中得到的通常的策略梯度进行训练的。经理使用作者介绍的过渡梯度进行训练。这个梯度是使用任务奖励以及分配给工人的子目标与工人实际状态转换之间的距离得出的。因此，管理者从任务奖励和工人的行为中学习。过渡梯度被用来学习管理者的政策和潜在的子目标空间。这种方法被称为封建网络（FuN），在Atari游戏上显示出比DQN[68]和Option-Critic[3]更好的性能，例如蒙特祖马的复仇。
% 封建网络并不保证所学的子目标空间会导致最优的分层策略。Nachum等人[70]在基于HIRO[69]开发的子目标表示学习方法中解决了这个问题（HIRO在第3.1.1节中讨论）。作者根据对分层策略的次优性的理论约束，得出了一个优化目标。这个目标被用来学习一个函数fθ（s），它将状态空间转换为低维的子目标空间。因此，学习到的子目标空间表示最小化了次优性，基于HIRO的分级策略解决了非平稳性问题。这种方法在MuJoCo连续控制任务上进行了评估[97]，同时使用了低维状态空间和高维状态空间（例如用作状态的图像）。它优于其他各种子目标表示方案，如直接使用原始状态（如图像）作为子目标，使用以FuN[99]风格学习的潜在空间，使用从变异自动编码器[47]衍生的子目标嵌入，等等。