\chapter{文献综述}



\section{强化学习}
\label{sec:review_rl}

\section{分布式强化学习}

深度强化学习（DRL）已经在一系列任务中取得了显著的成功，从游戏\cite{silver2016mastering,silver2017mastering}到现实世界的问题\cite{jumper2021highly}。更重要的是，大规模的强化学习已经能够在一些具有挑战性的游戏上达到与人类接近甚至超越人类的水平，如StartCraft 2\cite{vinyals2019grandmaster}和Dota\cite{berner2019dota}。DRL严重依赖试错和经验评估，需要智能体进行大量探索，这使得与游戏环境交互的时间成为了模型训练的瓶颈。因此，该领域的推进迫切需要分布式方法来提高探索效率。

同时，目前面向深度学习的分布式系统和算法取得了极大的进展，其中包括梯度下降算法的分布式形式以及分布式的深度学习框架\cite{goyal2017accurate, you2017scaling}，使得大规模的模型训练成为可能。因此，研究人员提出并开发了面向深度强化学习的分布式架构，使智能体能够更快地从经验中学习，利用探索策略，甚至有能力同时学习不同的任务集。

强化学习的特点在于，模型训练和数据采集是同时进行的。分布式强化学习旨在将两者并行化处理，以及为两者实现并行加速。因此如何确保数据的实效性，协调训练与模拟的速度，采样训练等，都是分布式强化学习算法需要解决的问题。

强化学习任务的问题定义，以及一些典型的强化学习算法已经在\ref{sec:review_rl}描述，本节的后续部分将探讨如何将这些算法进行分布式加速。本章节将只涉及单智能体算法，分布式多智能体算法不在讨论的范围。

\subsection{GORILA}

GORILA（The General Reinforcement Learning Architecutre）\cite{nair2015massively}是一个异步分布式强化学习架构。Gorila在算法上与DQN有相似之处，但有多个执行器和学习器，每个都在一台机器内运行，并在每个机器内存储一个Q网络。它利用DistBelief方法\cite{dean2012large}来计算随机梯度下降。

GORILA框架架构由4个主要部分组成，分别为参数服务器、执行器、学习器和经验回放缓冲区。参数服务器是Q网络参数$\theta$的真实来源，执行器和学习器各自存储一个$\theta$的副本，因此参数服务器试图使执行器和学习器的$\theta$保持最新。每个执行器都存储着参数$\theta$，根据它来采取行动。它只是用于行动，不参与随机梯度下降程序。所以它与参数服务器的更新同步。执行器将他们的经验$(s,a,s',a')$发送到经验回放缓冲区中，缓冲区可以放在一个独立的分布式数据库中，也可以放在执行器内部。学习器从缓冲器中抽取经验并执行随机梯度下降更新。在DQN中，每个学习器保持一个可学习的Q网络，参数为$\theta$，以及一个参数固定的目标Q网络，参数为$\theta^-$。目标网络在$N$次迭代中被更新一次，在DQN中就是如此。学习器的责任是根据从缓冲区采样的经验来计算损失。但是与DQN不同的是，学习器并不更新Q网络，而是将产生的梯度发送到参数服务器上。只有当参数服务器更新$\theta$时，改进的Q网络才会被同步，因为还有其他学习器在$\theta$上发布更新，而且发出的更新不一定有效。参数服务器包含一个DistBelief网络。如上所述，它负责根据来自学习器的传入梯度更新网络参数$\theta$，并将网络参数发送到学习器和执行器的Q网络。参数服务器丢弃“过期”的梯度更新，使得网络参数即使在进程和通信延迟和失败的情况下也是可靠的。这个架构的几个变体被归类为GORILA。学习器和执行器的数量是任意的，学习器和执行器可以是“捆绑”的，学习器和执行器数量相同，两两成对放在一台机器里。这个变体减少了通信开销，所以它的性能更好。

\subsection{A3C和A2C}

在GORILA中，执行器与学习器的数据传递通过经验回放缓冲区进行，这也限制了其适用范围仅为离轨策略（off-policy）算法。A3C（Asynchronous Advantage Actor-Critic）\cite{mnih2016asynchronous}框架为同轨策略（on-policy）算法进行了分布式结构设计。

A3C包含了一个全局的策略神经网络，以及多个智能体，每个智能体都有自己的神经网络参数。智能体与它的环境副本进行互动并使用获得的经验训练自己的神经网络。当智能体计算得到当前的神经网络梯度后，将这一梯度提交给全局的策略神经网络节点；该节点更新参数后将最新的神经网络参数返回给该智能体，以更新其网络参数。每个智能体与环境的交互和网络参数的更新都是异步的，这也导致了这种网络参数更新策略并不是最优的。

为了达到最优的网络更新效果，A2C（Advantage Actor-Critic）将全局策略神经网络的参数进行同步更新：接受当前迭代中来自所有智能体的梯度后统一更新并返回更新后的参数，然后在下一次迭代中，每个智能体从同样的策略开始。同步梯度更新解决了提到的不一致问题，在大批量的情况下效果更好，但却缺乏智能体的独立性。

\subsection{Ape-X和R2D2}

事实上，类似于GORILA的框架中，随着执行器数量的增加，模型能够收集足够多的训练数据。在这些样本中，已知的样本占绝大部分。价值神经网络已经能够较好的估计这些样本的价值，因此重复训练是冗余的。而对于在随机探索中获取的未知的样本相对稀少，而其价值不能被很好的估计。在实际采样中，我们需要根据重要性为这些样本分配采样权重，使得稀有的有效样本能够具备更大的被采样概率。优先经验回放缓冲区（Prioritized Experience Replay, PER）\cite{schaul2015prioritized}让优先级更高的样本更有概率被采集到，而优先级低的样本被采集的概率则降低。
% 算法定义每个样本的优先级为其时序差分误差（TD-error）。为保证所有样本都有概率被采集，优先级在原基础上加上了一个很小的值$\epsilon$。
PER算法的细节将在xxx描述并加以讨论。

论文\cite{horgan2018distributed}提出分布式强化学习框架Ape-X。其结构相比与GORILA更为简洁，移去了参数服务器，仅使用一个学习器消化来自所有执行器的样本。借助优先经验回放缓冲区，Ape-X在Atari电子游戏基线上获得了远超GORILA的实验结果。Ape-X框架以DQN算法为基础，组合了DQN的多种变体，包括双Q学习\cite{hasselt2010double, van2016deep}算法，多步回报时序差分估计，并使用了Dueling DQN网络结构\cite{wang2016dueling}作为价值函数近似器。为了解决连续动作空间的任务，Ape-X还实现了深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）\cite{lillicrap2015continuous}算法。

Ape-X的后续工作R2D2（Recurrent Experience Replay Distributed Reinforcement Learning）\cite{kapturowski2018recurrent}加入了循环神经网络RNN结构，扩展了马尔可夫决策过程的假设，使当前状态的价值估计由更长的历史状态和动作决定。对于R2D2来说，经验回放缓冲区存储的不再是$(s,a,r,s')$四元组，而是预设长度的$(s,a,r)$序列。其经验的优先级由序列中最大优先级和平均优先级共同决定。

\subsection{SEED RL}

在上述的所有分布式框架中，执行器一侧的策略神经网络推理都是在CPU上执行的，这比使用加速器的计算效率低很多。若是在执行器上部署GPU来加速推理，则会限制执行器的数量。随着网络模型变得庞大，这个问题变得更加关键。此外，每个执行器在一台机器上操作两个不同的任务：模型推理和环境渲染。这导致了低效益的资源利用。因为这两个任务并不相似。另一个问题是，这些分布式框架中，模型参数和经验轨迹在执行器和学习器之间传输，因此所需的带宽可能成为一个限制性因素。

SEED RL（Scalable and Efficient Deep-RL）\cite{espeholt2019seed}是一个高效的利用资源和加速器的分布式强化学习框架。其克服这些缺点的关键思想是，模型推理和轨迹收集是在学习器中进行的，而非在执行器。整个设计在某种程度上是一个具有远程环境的单机结构。因此我们可以使用如GPU和TPU的加速器实现批量推理。SEED RL的完整流程可以描述为：执行器将与环境交互获得的状态和奖励传递给学习器。学习器在聚合了一批状态后使用其策略神经网络实现批量推理，并将动作返回给各个执行器。同时学习器还将$(s,a,r,s')$四元组保存在本地存储用于网络训练。执行器在获得推理得到的动作后执行动作，进入下一轮迭代。

然而， 在这个模型中可能出现延迟问题。将推理放置在远端，当通信的时间开销超过本地推理的时间开销时，执行器则会出现延迟。为了减少这种延迟，SEED RL引入了一个基于gRPC（一个高性能的开源RPC框架）的高效库，包括异步流RPC（每个执行器与学习器的连接都保持开放）。在这个框架中，采用了一个批处理模块，以一种良好的组织方式对多个角色的推理调用进行批处理。此外，在行为体可以在同一台机器上的情况下，该框架利用Unix域套接字，因此可以最小化延迟和CPU开销。基于这一通信框架，SEED RL在单台机器上实现了每秒一百万次的查询，执行器的数量可以增加到数千台机器，学习器可以扩展到数千个核心，使其有可能以每秒数百万帧的速度训练。

\subsection{其他分布式强化学习框架}

A3C与A2C框架由于采用了同轨策略梯度算法Actor-Critic，因此其样本利用率较低。IMPALA\cite{espeholt2018impala}沿用了A3C的分布式架构，并在其上参考Retrace\cite{munos2016safe}加入了V-trace方法对历史轨迹得到的梯度进行同轨修正。这使得IMPALA能够维护一个较小规模的经验回放缓冲区，提升样本了利用率。

Distributed PPO（Proximal Policy Optimization）\cite{heess2017emergence}将PPO\cite{schulman2017proximal}算法进行扩展，得到了一个类似于A3C结构的分布式架构。

\section{分层强化学习}

强化学习智能体的目标是搜索最佳策略，使得在这一最佳策略之下智能体能够最大化累计奖励的期望值。在探索状态空间和动作空间以搜寻最优策略时，智能体将会得到各种轨迹。这些轨迹的期望长度被称作这一任务的视野（Horizon）。视野在强化学习理论中被定义为无限的，但是实际任务场景中通常是有限的。当状态空间和动作空间很大，任务的视野很远时，使用标准的强化学习方法进行探索就变得很有挑战性。

分层强化学习提供了一种机制，将具有挑战性的任务分解成更简单的子任务。在这样的层次结构中，最高级别的策略的动作空间通常被定义为选择一个子任务来分配给低级别策略执行。这个策略通过以合适的顺序选择子任务来完成主任务，以获得主任务的奖励。在层次结构的较低层次，又高层策略选择的子任务本身就是一个强化学习问题。底层策略学习使用与之相关的内在奖励来执行该子任务（也可选择加入主任务奖励）。最底层的策略的动作空间，即为主任务的动作空间。

在一种分类方案下，分层强化学习可以被分为封建分层结构，子策略选项结构，自主技能探索等类别。

\subsection{封建分层结构}

Dayan和Hinton于1993年提出了基础的封建分层强化学习结构\cite{NIPS1992_d14220ee}，被称为封建强化学习（Feudal Reinforcement Learning）。封建强化学习简要描述如下：一个较高层次的管理者设置了一个子任务，由较低层次的工人执行。这是一种成对的关系，如果层次结构有两层以上，工人就会成为它下面那一层的管理者。管理者通过一个子目标将子任务传达给工人，其中子目标是原始或抽象状态空间中的一个状态。工人的目标是要达到给定的子目标。每个级别的工人都是一个通用策略。任务奖励只有最高级别的管理者可以观察到，而其他级别的工人则利用达到子目标的奖励来学习。本文讨论的其他封建分层方法都是基于上述封建强化学习的概念。

分层DQN（Hierarchical DQN）\cite{kulkarni2016hierarchical}算法是一种深度分层强化学习算法，由分别代表管理者和工人的两层深度Q网络组成。管理者网络从一组预定义的子目标中选择一个子目标。它是根据环境奖励来学习的。工人的网络则是将子目标作为输入，并选择游戏环境动作空间中的动作实现子目标。它是根据子目标的奖励来学习的。一个子目标要么是原始状态，要么是状态的抽象表示。

工人智能体在学习过程中不断变化，也就意味着管理者的状态转换是非平稳的。为了在非平稳环境中实现数据的高效学习，HIRO（Hierarchical Reinforcement Learning with Off-policy Correction）\cite{nachum2018data}在两级分层结构下，提出将子目标重新标记以解决非平稳问题。在工人的历史数据中，包含了子目标$g_{old}$和其对应的轨迹$\tau$。而经过网络更新后的工人神经网络在$g_{old}$下将不再执行轨迹$\tau$。HIRO提出的离轨修正方案在分布中寻找一个新的子目标$g_{new}$，使得在这一子目标$g_{new}$下，当前策略能够得到与轨迹$\tau$最相似的轨迹。

与HIRO同时，HAC（Hierarchical Actor-Critic）\cite{levy2017learning}算法出现，它也通过子目标的重新标记来解决非平稳性问题。在HAC中，高层历史样本中的输出子目标和低层历史样本中的输入子目标被替换为智能体在事后\cite{andrychowicz2017hindsight}的实际状态，而不是像HIRO那样基于概率的抽样，抽出一个新的子目标。这种简单的方案也使层次结构扩展到两级以上成为可能。

所有层次的智能体，包括其子空间表示和策略是能够进行联合训练的。FuN\cite{vezhnevets2017feudal}是一个基于神经网络的封建分层结构，其中一个被称为管理者的较高层次网络在学习到的隐层子目标空间中对一个子目标进行采样。子目标可以是隐层空间中的一个点，也可以是代表隐层空间中的一个方向的单位向量。子目标被称为工人的神经网络作为输入，该网络必须学习一个策略来实现子目标，使用与子目标的距离作为奖励。管理者使用过渡梯度进行训练。这个梯度是使用任务奖励以及分配给工人的子目标与工人实际状态转换之间的距离得出的。因此，管理者从任务奖励和工人的行为中学习。过渡梯度被用来学习管理者的策略和隐层子目标空间。

FuN并不保证所学的子目标空间能得到最优的分层策略。Nachum等人\cite{nachum2018near}在基于HIRO\cite{nachum2018data}开发的子目标表示学习方法中解决了这个问题。作者根据对分层策略的次优性的理论约束，得出了一个优化目标。这个目标被用来学习一个函数$f_\theta(s)$，它将状态空间转换为低维的子目标空间。因此，学习到的子目标空间表示最小化了次优性，基于HIRO的分级策略解决了非平稳性问题。

\subsection{子策略选项结构}

Sutton等人\cite{sutton1999between}提出了选项（Options）框架，在该框架中，基本的马尔可夫决策过程（Marcov Decision Process，MDP）被扩展，其智能体的动作空间被扩展为子任务（或选项）集合。原始动作被看作单步的选项。选项$\omega$被定义为一个元组$(<I_\omega, \pi_\omega, \beta_\omega>)$。$I_\omega\subset S$是选项$\omega$的初始状态集合。$\beta_\omega:S\rightarrow[0,1]$表示选项$\omega$在某状态下终止的概率。$\pi_\omega:S\times A \rightarrow[0,1]$是该选项$\omega$的策略。与选项相关的奖励函数$r_\omega$可以存在，也可以是通用的。

通常我们假定选项是被预先定义的，它可以结合实际任务进行微调。这种微调方法被称作选项内学习。选项内学习使用的是实际任务奖励，即使我们提供了选项专有的奖励$r_w$。除此之外，每个选项的Q价值函数表示直到任务结束的累计回报，而非选项结束。因此，选项不是一个独立的子任务单元，而是被混合进了完整的MDP过程。这限制了选项的迁移能力，但是理论上保证了学到的分层策略的最优性。与之相反，MAXQ算法\cite{dietterich2000hierarchical}中每个选项的价值函数为累积到该选项结束的价值估计。这使得MAXQ在牺牲了最优性保证，但其选项能够简单的迁移到其他任务中。

Skill Chaining\cite{konidaris2009skill}是一种增量式构建选项的分层方案。在每一轮迭代中增加一个选项智能体，以增强其能力。在第一轮迭代中，算法创建一个选项$\omega$，并构建选项对应的子目标集合$g_w$，此集合$g_w$仅包含主任务的目标。该选项的策略$\pi_\omega$被训练以从任意初始状态开始抵达$g_w$中的子目标。训练完成后得到所有可以抵达$g_w$中的子目标的初始状态集合$I_\omega$。在下一步迭代中，新的选项$\omega'$被创建，其子目标集合$g_{\omega'}$即为上一轮迭代的可达初始状态集合$I_\omega$。经过多步迭代就可以得到一个链式的选项序列。这种方案能够得到任意数量的选项链。Skill Chaining算法的最大限制是它只有在目标被明确定义并且要求在存在某些初始状态，使得在这些状态的单个选项智能体就可以到达目标状态。

Skill Chaining的增量式的选项学习方法使得其在初期难以训练。为此Bacon等人\cite{bacon2017option}提出了Option Critic选项框架，它能够从学习过程的一开始就学习选项和整个分层策略。Option Critic随机地初始化固定数量的选项，每个选项都包含了策略神经网络和标志终止的函数。高层策略也是随机初始化的。所有的网络（高层策略、每个选项的策略和终止函数）都是通过使用主任务奖励得到的策略梯度学习的。Option Critic中并不包含子目标或者选项特定的奖励函数，并且理论上保证使用策略梯度学习最佳的分层策略。其主要局限性表现在预定义的选项数量。此外，策略梯度很大程度上取决于任务的奖励，这使得其在稀疏奖励任务上表现不佳。在Option Critic基础上，一些代表性的延续工作包括Proximal Policy Option Critic\cite{klissarov2017learnings}将其扩展到连续动作空间的任务中；Hiearchical Option Critic\cite{riemer2018learning}将其扩展到包含任意层的分层策略，Interest Option Ctiric\cite{khetarpal2019learning}则是加入了兴趣函数来学习选项的初始状态$I_\omega$。

\subsection{自主技能探索}

一些研究者希望采用能够直接发现一组不同技能的一般方法，而不是通过子目标来学习它们。技能指的是子任务的策略，即它在语义上代表了做好某件事的能力。在发现之后，技能可以作为低级策略被添加到HRL代理中，并在特定的任务中进行微调。

发现不同技能的一种策略是通过最大化技能$\omega$和使用该技能所能达到的状态或产生的轨迹之间的互信息（Mutial Information）。这两个变量的互信息被定义为通过观察其中一个变量能够获得关于另一个变量的信息量。对于一个隐层技能向量例如$z_\omega$，其对应的策略为$\pi_\omega(s)=\pi(s,z_\omega)$，其中$\pi(s,z_\omega)$是一个将技能向量作为输入的通用策略。最大化$z_\omega$和遵循$\pi(s,z_\omega)$所达到的状态或产生的轨迹之间的互信息，就可以发现一组与不同的终止状态或轨迹相关的不同技能。以下是基于这一策略的具体方法的概述。

算法SSN4HRL\cite{florensa2017stochastic}使用随机神经网络表示通用策略$\pi(s,z_\omega)$。这个策略通过最大化互信息奖励$P(z_\omega|(x,y))$来学习，其中$(x,y)$是智能体在空间环境中的位置，$P(z_\omega|(x,y))$是通过观察$(x,y)$预测$z_\omega$的概率。只有在执行不同技能所到达的位置足够不同的情况下，这个概率才能达到最大。Gregor等人\cite{gregor2016variational}提出了变分内在控制（Variational Intrinsic Control，VIC），它通过最大化包含技能向量$z_\omega$和策略$\pi(s,z_\omega)$达到的终端状态$s_T$之间的互信息项$P(z_\omega|s_0,s_T)$来发现技能，起始状态为$s_0$。Eysenbach等人\cite{eysenbach2018diversity}提出了一种名为Diverity Is All You Need（DIAYN）的方法，该方法通过优化一个目标函数来发现技能，该目标函数旨在最大化技能向量$z_\omega$和$\pi(s，z_\omega)$生成的轨迹中的每一个状态之间的互信息，而不考虑轨迹中的状态顺序。

所有这些方法都能发现一系列不同的技能，但也有一些共同的局限性，例如，它们只能发现预设数量的技能，而且它们的互信息目标要么忽略轨迹，要么不考虑轨迹中的状态顺序。为了解决这些局限性，Achiam等人\cite{achiam2018variational}提出了变分自编码强化学习（Variational Autoencoding Learning of Options by Reinforcement，VALOR）。在这种方法中，策略$\pi(s，z_\omega)$是一个编码器，它将$z_\omega$编码为一个轨迹。该轨迹被表示为$\tau$。它被送入一个解码器，该解码器必须将其映射回$z_\omega$。这一优化目标旨在使$P(z_\omega |\tau）$最大化。因此，VALOR发现了一组不同的技能，这些技能基本上与智能体所采取的轨迹的多样性相关。VALOR还提供了一个发现逐渐增加的技能数量的程序。

到目前为止讨论的自主技能探索方法只能发现一套离散的技能。一个可能的原因是，计算技能的连续空间的概率分布，以确定互信息，是很复杂的。然而，发现一个连续的技能空间对于泛化是很重要的，这样新的技能可以很容易地在这样一个连续的空间中插值。Co-Reyes等人\cite{co2018self}提出了一种叫做自洽轨迹自编码器（Self Consistent Trajectory Autoencoder，SeCTAR）的方法，其中一个编码器LSTM\cite{hochreiter1997long}将状态转换轨迹（即$\{s_t , s_{t+1}, s_{t+2} ...\}$）嵌入到一个低维的连续隐层向量空间，一个解码器LSTM学习将隐层向量解码为一个策略。隐层向量代表类似的轨迹，因此，从隐层向量解码的策略被认为是代表一种技能。使用能够生成多样化轨迹的探索机制，便可学习到多样化的隐层空间技能。这个编码器-解码器模型是在技能发现的预训练阶段学习的。预训练阶段结束后，编码器模块被移除，编码后的隐层空间可作为技能的连续空间。分层强化学习智能体的高层策略在这个连续空间中对技能进行采样，然后由解码器模块以策略的形式进行解码。

\section{小结}
